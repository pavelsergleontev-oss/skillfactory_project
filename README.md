# Мониторинг популяции Амурских тигров, при помощи фотоловушек и CLIP (Contrastive Language–Image Pretraining)
Проект в рамках программы машинное обучение, 1 семестр, проектная практика.  

На мой взгляд, данный проект имеет большую актуальность, так как амурские тигры находятся на грани вымирания. Амурский тигр или уссурийский тигр— популяция подвида тигра Panthera tigris tigris, населяющая Дальний Восток России и Северо-Восточный Китай, также существует информация о небольшой популяции в Северной Корее. Амурские тигры живут в охраняемой зоне на юго-востоке России, по берегам рек Амур и Уссури в Амурской области, Хабаровском и Приморском краях, Еврейской автономной области. Более всего амурские тигры распространены в предгорьях Сихотэ-Алинь в Лазовском районе Приморского края, где на сравнительно небольшой территории живёт каждый шестой дикий амурский тигр. Из-за антропогенного воздействия на окружающею среду, данный вид, до недавнего времени находился на грани вымирания, а на сегодняшний момент занесен в Красную книгу России. Исчезновение данного вида способно нанести большой удар по экосистеме тех регионов, где обитает данный вид, что в свою очередь может привести к различным экологическим последствиям. Таким образом, можно сделать вывод, о важности наблюдения и мониторинга за популяцией данного вида тигров со стороны человека с целью защиты его угрозы исчезновения.

На данный момент наблюдение за популяцией производится преимущественно при помощи фотоловушек, устанавливаемых в диких местах, для обработки полученных данных необходимы усилия специальных людей. Суть моего проекта в том, чтобы объединить уже отлаженное решение по фиксации разных особей тигров в разных регионах с моделью машинного обучения, которая способна упростить наблюдение.

На данном этапе предлагается использовать данные с фотоловушек для определения амурских тигром, так как фотоловушка реагирует на любое движение в области чувствительности специального датчика, зачастую под прицел ее объектива попадают различные дикике животные, наблюдение за которыми в данной ситуации не столь важно. При помощи предложенного метода предполагается в автоматическом режиме отделять изображения с амурским тигром от изображений без него. 

Для решения это задачи было решено использовать модель CLIP (Contrastive Language–Image Pretraining) от OpenAI. Она предназначена для совместной обработки изображений и текстовых описаний.
В отличие от классических моделей компьютерного зрения, CLIP:
-	не обучается на фиксированном наборе классов
-	может работать с произвольными текстовыми описаниями
-	поддерживает zero-shot классификацию, то есть распознавание объектов без дополнительного обучения

Модель CLIP состоит из двух основных частей:
- Визуальный энкодер
  -	принимает изображение на вход
  -	извлекает визуальные признаки
  -	в используемой версии применяется архитектура Vision Transformer (ViT)
- Текстовый энкодер
  -	принимает текстовое описание
  -	преобразует его в векторное представление
  -	использует архитектуру на основе трансформеров
    
Оба энкодера отображают изображение и текст в общее векторное пространство.
На данном этапе разработки, будет использован преимущественно визуальны энкодер.

CLIP обучалась на большом наборе пар (изображение — текстовое описание), собранных из открытых источников.
Процесс обучения:
-	модель получает правильные пары изображение–текст
-	старается приблизить их представления в векторном пространстве
-	одновременно отдаляет неверные пары
  
Такой подход называется контрастивным обучением.

В рамках данного проекта модель работает следующим образом:
1.	Загруженное изображение тигра преобразуется в вектор признаков
2.	Каждый текстовый класс (например, «a photo of an Amur tiger») также преобразуется в вектор
3.	Модель вычисляет сходство между изображением и каждым текстом
4.	Полученные значения нормализуются и интерпретируются как вероятности

Класс с наибольшей вероятностью считается наиболее подходящим описанием изображения.

Использование CLIP особенно актуально для задач экологии, потому что:
-	редкие виды животных часто отсутствуют в стандартных датасетах
-	модель не требует переобучения под конкретный вид
-	можно добавлять новые виды простым изменением текстового описания
-	подходит для анализа изображений с фотоловушек

Характеристики модели:
-	визуальный энкодер: Vision Transformer (ViT-B/32)
-	размер патча: 32×32 пикселя
-	версия openai/clip-vit-base-patch32
-	хорошее соотношение точности и скорости
  
Несмотря на преимущества, CLIP имеет ряд ограничений:
-	модель не выполняет детекцию или сегментацию объектов
-	не определяет отсутствие объекта напрямую
-	чувствительна к формулировке текстовых описаний

Эти ограничения учитываются при интерпретации результатов проекта

Модель CLIP представляет собой современный подход к анализу изображений, объединяющий компьютерное зрение и обработку естественного языка. Благодаря zero-shot классификации она позволяет распознавать редкие виды животных, такие как амурский тигр, без дополнительного обучения, что делает её перспективной для экологических исследований.

Примеры использования CLIP в проекте:
- В репозитории находится файл AT.webp, в этом файле изображения амруского тигра с фотоловушки в ночное время суток 


![AT](https://github.com/user-attachments/assets/5938f915-088d-4ae1-91de-de4aa7732004)

После того как модель получила эта озображение, она начинается работать с ним, в результате можно получить следуюзий результат: 
`-a photo of an Amur tiger: 0.389
 -a photo without a tiger: 0.306
 -a photo of a Bengal tiger: 0.293
 -a photo of a lion: 0.007
 -a photo of a leopard: 0.003
 -a photo of a domestic cat: 0.002`

 Предаставленные данные, это результат сравнения выделенных по фото векторов признаков с векторами текстового описания возможных животных из кода проекта:
  `labels = [
    "a photo of an Amur tiger",
    "a photo of a Bengal tiger",
    "a photo of a lion",
    "a photo of a leopard",
    "a photo of a domestic cat",
    "a photo without a tiger"
]`

Также есть возможность наглядно увидеть разпределение веротности наличии тигра на изображении, а также какой именно это тигр или вообще возможно, это домашний кот...
<img width="690" height="455" alt="image" src="https://github.com/user-attachments/assets/02e1805c-1773-4039-b01d-c727f226be29" />


Для болейшей простоты добавлен особый "labels" - "a photo without a tiger", на случай если на изображении не будет тигра или животного хоть как-то похожего на него. 


- Также в репозитории находится еще один файл для примера, именного AT_2.jpg, если подгрузить в модель данный файл, то получим следующие результаты при таких же "labels":
  
![AT_2](https://github.com/user-attachments/assets/89c3237d-bc96-4017-b103-a268f5b92d4b)
  `a photo of an Amur tiger: 0.814
a photo of a Bengal tiger: 0.171
a photo without a tiger: 0.015
a photo of a lion: 0.000
a photo of a leopard: 0.000
a photo of a domestic cat: 0.000`

И конечно, наглядное распределение вероятости: 
<img width="685" height="455" alt="image" src="https://github.com/user-attachments/assets/cd70f373-77dc-4299-a8fb-ede4f8f15582" />

Если внимательно посмотреть на результаты полученные от двух этих изображений, то можно увидеть, что данная модель хорошо работает с цветными фото, на первом фото, модель определила тигра как амурского, но также была большая вероятность, по ее мнению, наличия там бенгальского тигра и  полное отсутсвие какого-либо тигра вообще! В случае со вторым примером, модель четко определила, что это амурский тигр, а вероятность то, что это бенгальский тигр или на фото вообще нет тигра была сильно меньшей, чем в первом  случае. Таким образом, можно сделать вывод, о том, что для более коректной работы модели, необходимо ее дообучить на датасете с черно-белыми фото животных.

В дальнейшем в рамках развития поекта планируется: 
- дообучение модели на датасете из черно-белых фото животных
- интеграция в общую систему с фотоловушками
- возможное расширение функционала, которое будет включать мониторинг дургих, редких видов животных в различных регионах

Установка и использование:
- для использования проекта необходимо слонировать данный репозиторий при помощи команды `git clone` + URL данного репозитория.
- запустить находящийся в репозитории ноутбук MVP.ipynd в Google Colab
- далее необходимо последовательно выполнить блоки кода в ноутбуке MVP.ipynd
- в самом ноутбуке MVP.ipynd каждый блок кода снабжен комментариями, которые позволят запустить данный проект
- используемая в рамках проекта модель не требует токена авторизации  от huggingface.co

Сылка на модель: https://huggingface.co/openai/clip-vit-base-patch32
  









